\chapter{Text Representation}
% exploration analysis: (text clustering, ...) unsupervised learning because i don't have precedent knowledge
% Dopo aver fatto il processing sul testo si ottiene un insieme di descrittori, indici.
In un sistema di information retrieval i documenti devono essere rappresentati in un formato interno e ordinati per essere indicizzati.

\section{Bag Of Words}
Un modo semplice per rappresentare un testo è una matrice in cui sulle righe ci sono termini estratti dal corpus (vocabolario) e sulle colonne i documenti.

La \textbf{Bag Of Words (BOW)} è una rappresentazione del testo che descrive le occorrenze di parole in un documento.

\textbf{Incidence Matrix}: specifica la presenza di un termine in un ogni documento.

Ogni documento può essere rappresentato da un insieme di termini o da un vettore binario.

\begin{minipage}[c]{0.55\linewidth}
  \begin{tabular}{c >{\columncolor{yellow}}c c c c}
                                 & {\color{blue} \textbf{Doc1}} & {\color{blue} \textbf{Doc2}} & {\color{blue} \textbf{Doc3}} & {\color{blue} \textbf{Doc4}} \\
    {\color{red} \textbf{Term1}} & 1                            & 1                            & 1                            & 0                            \\
    {\color{red} \textbf{Term2}} & 0                            & 1                            & 1                            & 1                            \\
    {\color{red} \textbf{Term3}} & 0                            & 0                            & 1                            & 0
  \end{tabular}
\end{minipage}
\hfill
\begin{minipage}[c]{0.4\linewidth}
  $\text{Rappresentazione di Doc1}$
  \medskip

  $R1 = \{\text{Term1}, \text{Term2}, \text{Term3}\}$\\
  $R1 = \langle1, 0, 0\rangle$
\end{minipage}

\bigskip

\textbf{Count Matrix}: specifica la numero di occorrenze di un termine in ogni documento.

Un documento viene rappresentato da un vettore di occorrenze.

\begin{minipage}[c]{0.55\linewidth}
  \begin{tabular}{c >{\columncolor{yellow}}c c c c}
                                 & {\color{blue} \textbf{Doc1}} & {\color{blue} \textbf{Doc2}} & {\color{blue} \textbf{Doc3}} & {\color{blue} \textbf{Doc4}} \\
    {\color{red} \textbf{Term1}} & 57                           & 57                           & 71                           & 133                          \\
    {\color{red} \textbf{Term2}} & 4                            & 34                           & 17                           & 92                           \\
    {\color{red} \textbf{Term3}} & 232                          & 2                            & 10                           & 293
  \end{tabular}
\end{minipage}
\hfill
\begin{minipage}[c]{0.4\linewidth}
  $\text{Rappresentazione di Doc1}$
  \medskip

  $R1 = \langle157, 4, 232\rangle$
  \medskip
\end{minipage}

Le rappresentazioni vettoriali non considerano l'ordine delle parole nel testo.

\textbf{Bag Of Words con N-grammi}\\
Pro: cattura le dipendenze locali e l'ordine\\
Contro: incrementa la frequenza delle parole

\section{Zipf's Law}
Descrive la frequenza di un evento (parola) in un insieme in base al suo rank.

\textbf{rank}: posizione di un termine nell'ordine decrescente di frequenza dei termini in tutta la collezione.

La frequenza di una parola w, f(w) è proporzionale a 1/r(w).
%
\begin{align*}
   & f \propto \frac{1}{r} \quad\Rightarrow\quad f \cdot r = k \; \text{(costante)}                                      & \\
   & P_r = \frac{f}{N} = \frac{A}{r} \qquad \text{probabilità del termine di rank r, } \quad A = \frac{k}{N} \approx 0.1 &
\end{align*}

\section{Luhn's Analysis}
Generalmente termini con frequenza molto alta e molto bassa sono inutili per discriminare i documenti.

\begin{center}
  \bigskip
  \input{plots/luhn}
\end{center}

L'abilità delle parole di discriminare il contenuto di un documento è massimo nella posizione tra i due livelli di cut-off.

Vogliamo assegnare dei pesi ai termini tenendo conto di questi due fattori:
\begin{itemize}
  \item corpus-wide: alcuni termini portano più informazione riguardo al documento
  \item document-wide: non tutti i termini sono ugualmente importanti
\end{itemize}

\pagebreak

Andiamo a definire due frequenze:
\begin{itemize}
  \item Term Frequency (TF): frequenza di un termine all'interno di un documento
  \item Inverse Document Frequency (IDF): frequenza di un termine in tutta la collezione
\end{itemize}

Il peso di un termine deve essere proporzionale a TF e inversamente proporzionale a IDF.

La \textbf{term frequency} $tf_{t, d}$ è il numero di occorrenze del termine $t$ nel documento $d$ diviso il numero totale di termini nel documento.
\begin{align*}
  tf_{t, d} = \frac{f_{t,d}}{\sum_{t_i} f_{t_i, d}}
\end{align*}
%
Questa misura può essere normalizzata dividendo per la frequenza massima nel documento $d$ per essere confrontabile tra documenti diversi.
\begin{align*}
  ntf_{t, d} = \frac{f_{t,d}}{max_{t_i} f_{t_i, d}}
\end{align*}
%
La \textbf{inverse document frequency} $idf_t$ è la frazione inversa della frequenza di un termine in un documento in scala logaritmica.
\begin{align*}
  idf_t = log(N/df_t), \quad df_t \leq N
\end{align*}
dove $df_t$ è la \textbf{document frequency}, ovvero il numero di documenti che contengono il termine $t$, e $N$ il numero totale di documenti.

Infine possiamo calcolare la funzione TF-IDF come prodotto di TF e IDF.
\begin{align*}
  tf\text{-}idf_{t, d} = (tf_{t,d} / max_{ti} tf_{ti,d}) * log(N/df_t)
\end{align*}
Questa funzione rappresenta il peso del termine $t$ all'interno di un documento $d$.

\begin{itemize}
  \item termine comune in un documento $\rightarrow$ high tf $\rightarrow$ high weight
  \item termine raro nella collezione $\rightarrow$ high idf $\rightarrow$ high weight
\end{itemize}
