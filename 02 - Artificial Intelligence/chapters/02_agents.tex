\chapter{Agents}

Un \textbf{agente} è qualsiasi cosa che può essere vista come percepente del suo ambiente attraverso sensori e che agisce sull'ambiente attraverso degli attuatori.

Per \textbf{percezione} si intende un'insieme di input percettivi di un agente in un determinato istante.
Una \textbf{sequenza di percezione} è l'intera storia delle percezioni che l'agente ha percepito fino ad oggi.

La scelta dell'azione da compiere di un agente può dipendere dall'intera sequenza di percezione, ma non da qualcosa che non ha percepito.

Il comportamento di un agente è descritto da una \textbf{agent function} che mappa una sequenza di percezioni ad azioni.
L'agent function (astrazione) viene implementata da un \textbf{agent program} (implementazione).

\section{Rational Agent}
Quando un agente viene messo in un ambiente genera una sequenza di azioni in base alle percezioni che riceve.
Questa sequenza causa l'ambiente ad andare attraverso una sequenza di stati.

Se la sequenza di stati è \textbf{desiderabile}, allora l'agente ha performato bene. 
La desiderabilità è data da una \textbf{performance measure} che valuta una sequenza di stati dell'ambiente.

\subsection{Razionalità}
La \textbf{razionalità} di un agente dipende da quattro cose:
\begin{itemize}
  \item la performance measure che definisce criterio di successo
  \item la conoscenza a priori dell'ambiente da parte dell'agente
  \item le azioni che l'agente può eseguire
  \item la sequenza di percezione dell'agente
\end{itemize}

Per ogni possibile sequenza di percezione, un \textbf{agente razionale} dovrebbe scegliere un'azione che ci si aspetta massimizzi la performance measure,
date le evidenze fornite dalla sequenza di percezioni e da una qualsiasi conoscenza a priori.

\subsection{Onniscienza e Apprendimento}
Un agente \textbf{onnisciente} conosce i risultato attuale e agisce di conseguenza.

La razionalità massimizza la performance attesa, mentre la perfezione massimizza la performance attuale.

Una parte importante della razionalità è l'\textbf{information gathering}, ovvero fare delle azioni al fine di modificare le percezioni future.

Un agente razionale deve, oltre a raccogliere informazioni, \textbf{imparare} il più possibile da quello che percepisce. 
La configurazione iniziale di un agente può contenere della conoscenza a priori dell'ambiente che con l'esperienza può essere modificata e aumentata.

\subsection{Autonomia}
Un agente razionale dovrebbe anche essere \textbf{autonomo}, ovvero affidarsi unicamente alle sue percezioni e compensare ad una conoscenza a priori parziale o scorretta fornita 
dal suo progettatore. Dopo una sufficiente esperienza dell'ambiente il comportamento può diventare indipendente dalla conoscenza a priori. 

\section{Task Environment}
Per definire un agente è necessario determinare il suo \textbf{task environment} che è descritto da 
performance measure, ambiente, attuatori e sensori. 
Questi insieme di fattori può essere riassunto con l'acronimo \textbf{PEAS} (\textbf{P}erformance, \textbf{E}nvironment, \textbf{A}ctuators, \textbf{S}ensors).

\subsection*{Fully Observable vs Partially Observable}
Quando i sensori di un agente gli danno accesso allo stato completo 
dell'ambiente in ogni momento, allora diciamo che il task environment
è \textbf{completamente osservabile}. Se l'agente non ha sensori, l'ambiente è \textbf{osservabile}.

\subsection*{Competitive vs Cooperative}
Un ambiente multi-agente è detto \textbf{competitivo} se massimizzare la performance measure di un agente
minimizza la performance measure di un altro agente. Se invece tutte le performance measure vengono
massimizzate, allora l'ambiente è detto \textbf{cooperativo}. 

\subsection*{Deterministic vs Stochastic}
Se lo stato successivo dell'ambiente è completamente determinato dallo stato corrente e dall'azione eseguita dall'agente, allora diciamo che l'ambiente è \textbf{deterministico}.
Altrimenti è detto \textbf{stocastico}.

\subsection*{Uncertain vs Certain}
Diciamo che un ambiente è \textbf{incerto} se non è completamente osservabile o non deterministico

\subsection*{Episodic vs Sequential}
In un ambiente \textbf{episodico} l'esperienza dell'agente è divisa in più episodi. In ogni episodio l'agente
riceve delle percezioni ed esegue una singola azione. L'episodio successivo non dipende dalle azioni prese 
negli episodi precedenti. 

In un ambiente \textbf{sequenziale}, invece, la decisione attuale può incidere sulle decisioni future.

\subsection*{Static vs Dynamic}
Se l'ambiente si aggiorna alle decisioni un agente, allora diciamo che l'ambiente è \textbf{dinamico} per quell'agente. 
In caso contrario, è \textbf{statico}.

In un ambiente statico l'agente non deve osservare lo stato dell'ambiente per decidere l'azione da eseguire.   

\subsection*{Discrete vs Continuos}
Un ambiente è \textbf{discreto} se c'è un numero fissato di azioni e percezioni al suo interno. 
In caso contrario, è \textbf{continuo}.

\subsection*{Known vs Unknown}
In un ambiente \textbf{conosciuto} l'agente conosce il risultato di tutte le sue azioni sull'ambiente.
Altrimenti dovrà imparare come funziona per fare delle buone scelte.

\section{Agent Structure}
L'obiettivo dell'AI è progettare un agent program che implementa la agent function che mappa le percezioni ad azioni.
Il programma viene eseguito su un calcolatore con sensori e attuatori che costituiscono l'architettura.
%
\begin{align*}
  \text{Agente} = \text{Programma} + \text{Architettura}
\end{align*}

\subsection{Simple Reflex Agents}
Questo tipo di agente seleziona le azioni sulla base della percezione corrente, ignorando quelle passate.
L'ambiente deve essere completamente osservabile (fully-observable). 

\subsection{Model-based Reflex Agents}
L'agente mantiene uno stato interno che dipende dallo storico delle percezioni. 
Questo riflette in qualche modo gli aspetti non osservabili nello stato corrente.

Per aggiornare lo stato interno è necessario conoscere come l'ambiente evolve indipendentemente dall'agente
e come l'azione dell'agente influenza l'ambiente.

\subsection{Goal-based agents}
Non sempre lo stato corrente dell'ambiente è sufficiente per decidere la prossima azione.
L'agente ha bisogno di una informazione sull'obiettivo che descrive la situazione desiderabile.

Quando per raggiungere l'obiettivo, l'agente deve considerare una lunga sequenza di azioni sono necessarie
delle fasi di ricerca e pianificazione.

\subsection{Utility-based agents}
L'obiettivo fornisce solo un'informazione binaria che nella maggior parte degli ambiente non basta 
per avere comportamenti complessi.

Si utilizza quindi una misura di performance più generale che permette di valutare gli stati dell'ambiente
sulla base di quanto è utile.

Se un agente ha più obiettivi, con la funzione di utilità è possibile gestire conflitti tra essi e pesarli in base 
all'importanza.

Un agente razionale utility-based deve modellare e tenere traccia del suo ambiente e dei suoi task.

\subsection{Learning agents}
Questa architettura permette all'agente di operare in ambienti sconosciuti e di diventare più competente.
Inoltre non è necessario definire manualmente l'agent program.
